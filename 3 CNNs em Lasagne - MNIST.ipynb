{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Redes Neurais Convolucionais usando Lasagne\n",
    "\n",
    "Nesse tutorial iremos treinar redes neurais convolucionais (CNNs) usando a biblioteca Lasagne, para o problema de classificação de dígitos MNIST.\n",
    "\n",
    "Esse tipo de rede neural é computacionalmente intenso, em particular rodando em CPUs, que é o caso para esse tutorial. Por isso, nesse tutorial começaremos com redes neurais \"tradicionais\" (fully-connected), para aprender sobre a biblioteca, e ganhar intuição sobre como treinar os modelos / como selecionar hyper-parametros, etc.\n",
    "\n",
    "Vamos começar carregando as bibliotecas, e a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "theano.config.floatX = 'float64'\n",
    "\n",
    "from helpers import mnist_data  # Função para carregar a base MNIST\n",
    "import matplotlib.pyplot as plt # Para visualizações\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = mnist_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Alguns exemplos:\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(), cmap='Greys', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('Primeiro digito da base de treinamento')\n",
    "\n",
    "f, ax = plt.subplots(4,8)\n",
    "for i in range(4):\n",
    "    for j in range(8):\n",
    "        img = x_train[i*8+j].squeeze()\n",
    "        ax[i,j].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "        ax[i,j].axis('off')\n",
    "plt.suptitle('Exemplos da base de treinamento', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tamanho da base de treinameto\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos  gerais\n",
    "Notamos acima que possuímos 50 mil exemplos para treinamento, onde cada imagem é de tamanho 28x28 e contém um único dígito. \n",
    "\n",
    "A tarefa é um problema de classificação entre 10 classes (dígitos de 0 a 9). Vamos criar modelos resolver o problema de estimar a classe correta, isto é, modelos que estimem: \n",
    "$$\\hat{\\textbf{y}} =P(\\textbf{y} | X) $$\n",
    "\n",
    "Onde $\\hat{\\textbf{y}}$ é um vetor de 10 posições, consistindo na probabilidade da imagem X pertencer a cada uma das 10 classes. Consideramos a decisão do classificador como o dígito mais provável, isto é:\n",
    "\n",
    "\n",
    "$$ \\DeclareMathOperator*{\\argmax}{arg\\,max} y_\\text{pred} = \\argmax_i{\\hat{y_i}}$$\n",
    "\n",
    "Para avaliar o classificador, calculamos a taxa de erro como a fração de exemplos que são classificados incorretamente:\n",
    "\n",
    "$$ \\text{error} = \\frac{1}{N}\\sum_i{ y_\\text{pred} \\neq y_\\text{true}}$$\n",
    "\n",
    "Vamos começam modelando $P(\\textbf{y} | X)$ usando arquiteturas mais simples (regressão logística), e progredindo até treinar redes neurais convolucionais de múltiplas camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede neural com apenas 1 camada (i.e. regressão logística)\n",
    "\n",
    "Como primeiro exercício, vamos criar um modelo com apenas uma camada (a de saída). Esse modelo é equivalente ao modelo de regressão logística que utilizamos ontem, com a diferença que o problema é de múltiplas classes (10 dígitos possíveis)\n",
    "\n",
    "Ontem, no exemplo com duas classes, usamos apenas um neurônio, com saída entre $[0,1]$ que estimava a probabilidade da classe $y=1$. Para 10 classes, vamos agora considerar 10 saídas $y_i$. Para ser uma distribuição de probabilidates, precisamos que essas saídas satisfaçam as seguintes propriedades:\n",
    "\n",
    "$$y_i \\ge 0 \\qquad \\sum_i{y_i} = 1$$\n",
    "\n",
    "Para isso, ao invés de utilizarmos a função sigmoid, utilizamos a função **softmax** na última camada:\n",
    "\n",
    "$$y_i = \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_k{e^{z_k}}}$$\n",
    "\n",
    "O uso do função expoente garante que $y_i > 0$, e o denominador garante que as probabilidades serão normalizadas: $\\sum_i{y_i} = 1$.\n",
    "\n",
    "Os passos que vamos tomar para esse exercício são:\n",
    "1. Definir uma arquitetura \n",
    "2. Definir uma função de custo\n",
    "3. Escolher um algoritmo de otimização\n",
    "4. Compilar a função de treinamento\n",
    "5. Chamar a função de treinamento até convergência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse primeiro exercício, não utilizaremos convoluções. Nesse caso, será mais fácil tratar cada exemplo da base de treinamento como um vetor de tamanho 28 * 28 = 784 dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(50000, 28*28)\n",
    "x_valid_flat = x_valid.reshape(10000, 28*28)\n",
    "\n",
    "print 'Tamanho da base de treinamento: ', x_train_flat.shape\n",
    "print x_train_flat.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1.1 - Definição da arquitetura\n",
    "\n",
    "Nesse exercício vamos criar uma arquitetura, usando lasagne, que contenha:\n",
    "\n",
    "* Camada de entrada (InputLayer), considerando o tamanho de cada exemplo (número de dimensões em X)\n",
    "* Camada de saída, fully-connected (DenseLayer), considerando o tamanho do vetor de saída, e usando a não-linearidade softmax\n",
    "\n",
    "Nota 1: Para detalhes de quais parâmetros passar para a criação das camadas, verificar: http://lasagne.readthedocs.io/en/latest/modules/layers.html (ou os slides da apresentação de hoje)\n",
    "\n",
    "Nota 2: Vamos treinar o modelo em \"batches\" que podem ter tamanho variado. Para isso, podemos deixar a primeira dimensão da entrada como \"None\"\n",
    "\n",
    "Nota 3: Os parâmetros W e b podem ser inicializados com a função padrão usado pelo Lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "net = {}\n",
    "\n",
    "net['data'] =  ##coloque aqui a definição da camada de entrada\n",
    "net['out'] = ##coloque aqui a definição da camada de saída "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verificando os tamanhos de entrada e saída:\n",
    "assert net['data'].shape == (None, 784), 'Entrada deveria ter dimensão 784, e tamanho de batch variável (None, 784)'\n",
    "assert net['out'].output_shape == (None, 10)\n",
    "assert net['out'].nonlinearity == softmax,'Saída deveria utilizar a função softmax'\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_nohid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1.2 - Definição da função de custo\n",
    "\n",
    "Para problemas de classificação, a função de custo apropriada é conhecida como \"cross-entropy\": \n",
    "\n",
    "$$L = - \\frac{1}{N}\\sum \\log P(y | x)$$\n",
    "\n",
    "Essa é a mesma função de custo que utilizamos na regressão logística de duas classes, com a diferença que agora estamos considerando 10 classes ao invés de uma única saída da rede.\n",
    "\n",
    "Essa função está implementada no lasagne: ```lasagne.objectives.categorical_crossentropy``` [manual](http://lasagne.readthedocs.io/en/latest/modules/objectives.html#lasagne.objectives.categorical_crossentropy)\n",
    "\n",
    "Para calcular a função de custo, precisamos:\n",
    "* Definir uma variável do Theano que representa a entrada (nesse exemplo, vamos usar a variavel que o lasagne criou automaticamente) \n",
    "* Obter a saída da Rede ($\\hat{\\textbf{y}} = P(\\textbf{y}|X)$) usando a função ```lasagne.layers.get_output``` [manual](http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.get_output)\n",
    "* Definir uma variável do Theano que representa a saída desejada ($y_\\text{true}$ - vamos chamar de \"output_var\")\n",
    "* Calcular o custo (erro) entre a saída da rede e a saída desejada, usando a função ```categorical_crossentropy```\n",
    "\n",
    "\n",
    "Dica: Como estamos definindo o modelo para ser treinado com vários exemplos ao mesmo tempo, a entrada é uma matrix e a saída esperada é um vetor de inteiros (T.ivector)\n",
    "\n",
    "Dica 2: A função ```categorical_crossentropy``` também retornará um vetor (o erro referente a cada exemplo de entrada). Precisamos então combinar todos os erros, tomando sua média: ```loss = loss.mean()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.objectives import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "input_var = net['data'].input_var\n",
    "predicted =  ## Obter a saída da rede\n",
    "\n",
    "output_var = ## Criação da variável simbólica de saída\n",
    "loss =  ## Cálculo da função de custo\n",
    "\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vamos considerar também a taxa de acerto\n",
    "\n",
    "y_pred = T.argmax(predicted, axis=1)\n",
    "acc = T.eq(y_pred, output_var)\n",
    "acc = acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Definição do algoritmo de treinamento\n",
    "\n",
    "No exemplo de regressão logística, utilizamos o algoritmo de descida de gradiente (Gradient Descent). No entanto, existem outros algoritmos de otimização que convergem mais rápido, como nesterov_momentum e adam. Lasagne possui vários deles implementados: http://lasagne.readthedocs.io/en/latest/modules/updates.html\n",
    "\n",
    "Nesse exemplo, vamos continuar usando descida de gradiente (em lasagne, chamado ```lasagne.updates.sgd```). Essa função é chamada da seguinte forma:\n",
    "\n",
    "```\n",
    "updates = lasagne.updates.sgd(loss, params, lr)\n",
    "```\n",
    "\n",
    "Onde os parâmetros são:\n",
    "* loss: a função de custo a ser otimizada\n",
    "* parametros: uma lista de parâmetros do modelo\n",
    "* lr: Learning Rate: o tamanho do passo a ser dado em cada etapa\n",
    "\n",
    "A saída dessa função (**updates**), é uma lista de pares do tipo [(variavel, expressão)], que define como cada variável é atualizada. Esse método implementa exatamente a mesma função que usamos ontem (por exemplo, se tivermos apenas dois parametros **w** e **b**, a função retorna:\n",
    "\n",
    "```\n",
    "updates = [\n",
    "    (w, w - lr  * w_grad),\n",
    "    (b, b - lr * b_grad)\n",
    "]\n",
    "```\n",
    "\n",
    "Para obtermos a lista de todos os parâmetros do modelo, utilizamos a função:\n",
    "\n",
    "```\n",
    "params = lasagne.layers.get_all_params(ultima_camada)\n",
    "```\n",
    "\n",
    "Onde passamos como parâmetro a última camada do modelo, e é retornada uma lista de todos os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "params = lasagne.layers.get_all_params(net['out'])\n",
    "updates = lasagne.updates.sgd(loss, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Compilando a função de treinamento\n",
    "\n",
    "Da mesma forma que no exercício de ontem, compilamos a função de treinamento para retornar o erro na base de treinamento, e também atualizar os parâmetros. Dessa forma, cada chamada à função de treinamento atualiza os parâmetros para reduzir o erro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, output_var], [loss, acc], updates=updates)\n",
    "val_fn = theano.function([input_var, output_var], [loss, acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1.5 - Treinamento\n",
    "\n",
    "Nesse exercício, vamos fazer o treinamento do modelo defindo acima, chamando a função de treinamento várias vezes, usando todos os dados de treinamento (```x_train_flat, y_train```).\n",
    "\n",
    "Para monitorar o progresso, vamos também calcular o erro na base de validação (```x_valid_flat, y_valid```), chamando a função de validação (```val_fn```), que retorna o erro, mas não utiliza os dados para alterar o modelo. Vamos também salvar o histórico de custo (em treinamento e validação) para depois monitorarmos o progresso do aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua Solução:\n",
    "cost_history = []   # Insira o custo do treinamento (a cada chamada) nessa lista. \n",
    "acc_history = []    # Taxa de acerto do treinamento\n",
    "val_cost_history = [] # Custo da base de validação\n",
    "val_acc_history = [] # Taxa de acerto na base de validação\n",
    "\n",
    "for i in range(50):    \n",
    "    ## Coloque aqui o código para chamar a função de treinamento (salvando o custo \n",
    "    ##   e taxa de acerto de cada chamada), e a chamada à função de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_trainloop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando a curva de aprendizado\n",
    "\n",
    "O código acima fez o treinamento do modelo por 50 iterações. Vamos agora visualizar o progresso do treinamento (valor da função de custo ao longo do tempo): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(cost_history, 'b--', label='Treinamento')\n",
    "plt.plot(val_cost_history, 'r-', label='Validacao')\n",
    "plt.xlabel('Numero de iteracoes', fontsize=15)\n",
    "plt.ylabel('Custo', fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que o erro, tanto em treinamento como em validação diminui ao longo do tempo - então o treinamento está funcionando. No entanto, notamos que o erro ainda está diminuindo ao fim das 50 iterações, então poderíamos continuar o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxa de acerto na base de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_acc_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refatorando o código\n",
    "\n",
    "No exercício acima, fizemos a definição da arquitetura e todo o código para treinamento usando variáveis globais (em uma única função). Na prática, vamos querer avaliar diferentes tipos de arquiteturas, modificar a função de custo ou o loop de treinamento, então é conveniente separar essas funcionalidades em diferentes métodos.\n",
    "\n",
    "Vamos considerar três funções:\n",
    "\n",
    "```\n",
    "net = build_no_hid_layer()\n",
    "``` \n",
    "\n",
    "> Definição de arquitetura (retorna um dicionário \"net\" com a definição da rede)\n",
    "\n",
    "```\n",
    "train_fn, val_fn = compile_train_function(net, lr)\n",
    "```\n",
    "\n",
    "> Dado uma arquitetura e learning rate, compila e retorna a função de treinamento e validação\n",
    "\n",
    "```\n",
    "training_curves = train(train_fn, val_fn, train_set, valid_set, epochs)\n",
    "```\n",
    "\n",
    "> Dado as funções de treinamento e validação; as bases de treinamento de validação e o número de \"epochs\" (iterações na base de dados), realiza o treinamento da rede, e retorna as curvas de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_no_hid_layer():\n",
    "    net = {}\n",
    "\n",
    "    net['data'] = InputLayer((None, 28*28))\n",
    "    net['out'] = DenseLayer(net['data'], 10, nonlinearity=softmax)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_train_function(net, lr):\n",
    "    input_var = net['data'].input_var\n",
    "    output_var = T.ivector()\n",
    "\n",
    "    predicted = lasagne.layers.get_output(net['out'], inputs=input_var)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predicted, output_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    y_pred = T.argmax(predicted, axis=1)\n",
    "    acc = T.eq(y_pred, output_var)\n",
    "    acc = acc.mean()\n",
    "\n",
    "    params = lasagne.layers.get_all_params(net['out'])\n",
    "    updates = lasagne.updates.sgd(loss, params, lr)\n",
    "\n",
    "    train_fn = theano.function([input_var, output_var], [loss, acc], updates=updates)\n",
    "    val_fn = theano.function([input_var, output_var], [loss, acc])\n",
    "    return train_fn, val_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_fn, val_fn, train_set, valid_set, epochs):\n",
    "    x_train, y_train = train_set\n",
    "    x_valid, y_valid = valid_set\n",
    "    \n",
    "    cost_history = []\n",
    "    acc_history = []\n",
    "    val_cost_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        cost, acc = train_fn(x_train, y_train)\n",
    "        cost_history.append(cost)\n",
    "        acc_history.append(acc)\n",
    "\n",
    "        val_cost, val_acc = val_fn(x_valid, y_valid)\n",
    "        val_cost_history.append(val_cost)\n",
    "        val_acc_history.append(val_acc)\n",
    "    return cost_history, acc_history, val_cost_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train_curves(train_curves):\n",
    "    plt.figure()\n",
    "    cost_history, acc_history, val_cost_history, val_acc_history = train_curves\n",
    "    plt.plot(cost_history, 'b--', label='Treinamento')\n",
    "    plt.plot(val_cost_history, 'r-', label='Validacao')\n",
    "    plt.xlabel('Numero de iteracoes', fontsize=15)\n",
    "    plt.ylabel('Custo', fontsize=15)\n",
    "    plt.legend()\n",
    "    print('Taxa de acerto em Validação: %.2f%%' % (val_acc_history[-1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o experimento com as funções refatoradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_no_hid_layer()\n",
    "\n",
    "train_fn, valid_fn = compile_train_function(model, lr=0.01)\n",
    "\n",
    "train_curves = train(train_fn, valid_fn, \n",
    "                     train_set=(x_train_flat, y_train), \n",
    "                     valid_set=(x_valid_flat, y_valid),\n",
    "                     epochs=50)\n",
    "\n",
    "plot_train_curves(train_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2: Otimizando a Learning Rate\n",
    "\n",
    "Apesar de o modelo de regressão logística não possuir nenhum hiper-parâmetro, o processo de treinamento usa o hiper-parâmetro \"Learning Rate\" (tamanho do passo da descida de gradiente). Esse hiper-parâmetro é um dos mais importantes a serem otimizados: se o passo for muito grande, o treinamento pode divergir (i.e. aumentar o erro ao invés de diminuir). Se o passo for muito pequeno, o número de iterações necessárias para convergência aumenta.\n",
    "\n",
    "Nesse exercício, o objetivo é treinar modelos e comparar as curvas de treinamento usando learning rate variando entre:\n",
    "\n",
    "$$\\text{lr} \\in \\{0.01, 0.1, 1, 10\\}$$\n",
    "\n",
    "Devido ao custo computacional, vamos limitar o número de iterações (epochs) em 10 nesse exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução: crie 4 modelos, treine cada um por 10 epochs com differentes learning rates, \n",
    "#           e use a função plot_train_curves para visualizar as curvas de treinamento de cada uma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_lr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que usando $\\text{lr} = 0.01$ o modelo demora demais para convergir. No entanto, $\\text{lr} = 10$ diverge, e não parece estável. \n",
    "\n",
    "As melhores opções estão entre $\\text{lr} = 0.1$  e $\\text{lr} = 1$. Para o resto dos experimentos, vamos usar o valor de 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "Nos exemplos acima, usamos toda a base de treinamento para cada atualização dos pesos do modelo. Esse método é conhecido como \"Batch gradient descent\". No entanto, nota-se na prática que se obtem convergência mais rápida usando um algoritmo conhecido como \"Stochastic Gradient Descent\": ao invés de utilizar todos os exemplos da base de treinamento para calcular a função de custo, e em seguida atualizar os pesos, esse algoritmo utiliza apenas alguns exemplos (um \"mini-batch\") para atualizar os pesos. Dessa forma, a cada iteração sobre a base de treinamento, a função de custo é avaliada várias vezes, e os pesos atualizados várias vezes.\n",
    "\n",
    "O código para o treinamento é semelhante ao do Batch Gradient descent, mas nesse caso chamamos a função de treinamento a cada mini-batch de exemplos. É comum definir uma função auxiliar (```iterate_minibatches```) que percorre a base de dados, retornando um mini-batch a cada iteração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(x, y, batch_size):\n",
    "    for batch_start in xrange(0, len(x), batch_size):\n",
    "        yield x[batch_start:batch_start+batch_size], y[batch_start:batch_start+batch_size]\n",
    "\n",
    "\n",
    "def train_minibatch(train_fn, val_fn, train_set, valid_set, epochs, batch_size):\n",
    "    x_train, y_train = train_set\n",
    "    x_valid, y_valid = valid_set\n",
    "    \n",
    "    cost_history = []\n",
    "    acc_history = []\n",
    "    val_cost_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    print('epoch\\ttrain_err\\tval_err')\n",
    "    for i in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        epoch_acc = 0\n",
    "        train_batches = 0\n",
    "        for x_batch, y_batch in iterate_minibatches(x_train, y_train, batch_size):\n",
    "            cost, acc = train_fn(x_batch, y_batch)\n",
    "            epoch_cost += cost\n",
    "            epoch_acc += acc\n",
    "            train_batches += 1\n",
    "\n",
    "        val_epoch_cost = 0\n",
    "        val_epoch_acc = 0\n",
    "        val_batches = 0\n",
    "        for x_batch, y_batch in iterate_minibatches(x_valid, y_valid, batch_size):\n",
    "            val_cost, val_acc = val_fn(x_batch, y_batch)\n",
    "            val_epoch_cost += val_cost\n",
    "            val_epoch_acc += val_acc\n",
    "            val_batches += 1\n",
    "            \n",
    "        epoch_cost = epoch_cost / train_batches\n",
    "        cost_history.append(epoch_cost)\n",
    "        acc_history.append(epoch_acc / train_batches)\n",
    "\n",
    "        val_epoch_cost = val_epoch_cost / val_batches\n",
    "        val_cost_history.append(val_epoch_cost)\n",
    "        val_acc_history.append(val_epoch_acc / val_batches)\n",
    "        print('%d\\t%.4f   \\t%.4f' % (i+1, epoch_cost, val_epoch_cost))\n",
    "    return cost_history, acc_history, val_cost_history, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando Batch vs Stochastic Gradient Descent\n",
    "\n",
    "Vamos comparar os dois algoritmos de treinamento, treinando-os por 10 epochs (10 iterações na base de dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model = build_no_hid_layer()\n",
    "train_fn, valid_fn = compile_train_function(model, lr=0.1)\n",
    "train_curves = train(train_fn, valid_fn,     # Treinamento usando Batch Gradient Descent\n",
    "                     train_set=(x_train_flat, y_train), \n",
    "                     valid_set=(x_valid_flat, y_valid),\n",
    "                     epochs=10)\n",
    "end = time.time()\n",
    "plot_train_curves(train_curves)\n",
    "print(\"Tempo de treinamento: %.2f segundos\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = build_no_hid_layer()\n",
    "train_fn, valid_fn = compile_train_function(model, lr=0.1)\n",
    "train_curves = train_minibatch(train_fn, valid_fn,      # Treinamento usando mini-Batch Gradient Descent\n",
    "                     train_set=(x_train_flat, y_train), \n",
    "                     valid_set=(x_valid_flat, y_valid),\n",
    "                     epochs=10,\n",
    "                     batch_size=128)\n",
    "plot_train_curves(train_curves)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Tempo de treinamento: %.2f segundos\" % (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver uma convergência muito mais rápida usando a versão estocástica, usando o mesmo número de iterações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3: Rede Neural com uma camada escondida\n",
    "\n",
    "Nesse exercício, vamos criar uma rede neural com uma camada escondida (fully-connected). Para tanto, vamos definir três camadas:\n",
    "\n",
    "* Camada de entrada\n",
    "* Camada escondida (Dense Layer)\n",
    "* Camada de saída (Dense Layer com não-linearidade softmax)\n",
    "\n",
    "Nota: Nesse exerício vamos usar a não-linearidade padrão (ReLU) na camada escondida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "def build_hid_layer(nhid):\n",
    "    net = {}\n",
    "    ## Coloque aqui a definição da rede. Use o parametro nhid como o número de neuronios na camada escondida\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_hid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3.2 Otimizando o número de neurônios na camada escondida\n",
    "\n",
    "Nesse exercício, vamos buscar o melhor número de neurônios na camada escondida.\n",
    "\n",
    "O objetivo é treinar a rede definida acima usando $\\text{nhid} \\in \\{10, 100, 1000\\}$. \n",
    "Vamos usar treinamento usando mini-batches, com learning rate 0.1, batch_size=128, e treinando por 30 epochs cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sua solucao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_hid_nhid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que os modelos com 100 e 1000 neuronios na camada escondida tem melhor performance (veja que o modelo com 10 neuronios performance muito pior apesar do gráfico ser parecido - note o eixo y) Notamos também que esses modelos começam a entrar em over-fitting - notando que o erro na base de treinamento continua a diminuir, mas o erro na base de validação continua constante.\n",
    "\n",
    "Notamos também que a performance (98% de acerto já é bem superior à performance sem nenhuma camada escondida (92%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Convolucionais\n",
    "\n",
    "Vamos considerar agora o treinamento de redes neurais convolucionais.\n",
    "As mudanças são:\n",
    " * A arquitetura da rede, que vai incluir camadas convolucionais\n",
    " * A entrada da rede, que terá formato (None, 1, 28, 28) ao invés de (None, 784)\n",
    " \n",
    "O tamanho da entrada (None, 1, 28, 28) segue o padrão do lasagne: a primeira dimensão é o número de exemplos no mini-batch (Usamos \"None\" para ser variável), a segunda dimensão é o número de canais na imagem (usamos 1 porque a imagem é em escala de cinza; usaríamos 3 para RGB); a terceira e quarta dimensões são altura e largura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4.1 - Rede convolucional pequena\n",
    "\n",
    "Vamos começar com uma rede pequena:\n",
    "\n",
    "* Camada de entrada\n",
    "* Camada convolucional, com 6 filtros de tamanho 5x5\n",
    "* Camada de max-pooling, de tamanho 3x3\n",
    "* Camada de saída, Dense Layer, usando não-linearidade softmax\n",
    "\n",
    "Dica: Utilize as classes Conv2DLayer e MaxPool2DLayer. http://lasagne.readthedocs.io/en/latest/modules/layers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer, MaxPool2DLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sua solução\n",
    "\n",
    "def build_conv_small():\n",
    "    net = {}\n",
    "    ## Coloque aqui a definição da rede\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solução\n",
    "\n",
    "%load solutions/cnn_convsmall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_conv = build_conv_small()\n",
    "train_fn, valid_fn = compile_train_function(model_conv, lr=0.1)\n",
    "train_curves = train_minibatch(train_fn, valid_fn, \n",
    "                     train_set=(x_train, y_train), \n",
    "                     valid_set=(x_valid, y_valid),\n",
    "                     epochs=30,\n",
    "                     batch_size=128)\n",
    "plot_train_curves(train_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que a performance com a rede convolucional pequena é equivalente à performance usando uma rede com 100 unidades na camada escondida. No entanto, as duas redes possuem  um número bem diferente de parâmetros.\n",
    "\n",
    "Podemos usar a função ```lasagne.layers.count_params``` para calcular quantos parâmetros são definidos no modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_conv = build_conv_small()\n",
    "lasagne.layers.count_params(model_conv['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_hid100 = build_hid_layer(100)\n",
    "lasagne.layers.count_params(model_hid100['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos então que a rede convolucional obteve a mesma performance usando apenas 4 mil parametros (vs 80 mil parametros no modelo fully-connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4.2 - Rede convolucional maior\n",
    "\n",
    "Nesse exercício, vamos construir uma rede convolucional com mais camadas:\n",
    "\n",
    "* Camada de entrada\n",
    "* Camada convolucional, com 8 filtros de tamanho 5x5\n",
    "* Camada de max-pooling, de tamanho 2x2\n",
    "* Camada convolucional, com 16 filtros de tamanho 5x5\n",
    "* Camada de max-pooling, de tamanho 3x3\n",
    "* Camada fully connected (Dense Layer) com 100 neurônios\n",
    "* Camada de saída, Dense Layer, usando não-linearidade softmax\n",
    "\n",
    "Dica: Utilize as classes Conv2DLayer e MaxPool2DLayer. http://lasagne.readthedocs.io/en/latest/modules/layers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "def build_conv_larger():\n",
    "    ## Coloque aqui a definição da rede\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute essa celula para ver a solucao\n",
    "\n",
    "%load solutions/cnn_convlarger.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos verificar o número de parâmetros nessa rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_conv_larger = build_conv_larger()\n",
    "lasagne.layers.count_params(model_conv_larger['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_conv_larger = build_conv_larger()\n",
    "train_fn, valid_fn = compile_train_function(model_conv_larger, lr=0.1)\n",
    "train_curves = train_minibatch(train_fn, valid_fn, \n",
    "                     train_set=(x_train, y_train), \n",
    "                     valid_set=(x_valid, y_valid),\n",
    "                     epochs=30,\n",
    "                     batch_size=128)\n",
    "plot_train_curves(train_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, obtemos uma performance próxima a 99%. Nessa base de dados, redes convolucionais não apresentam resultados tão superiores (o resultado de uma simples rede fully-connected já é bem elevado).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando e carregando modelos treinados\n",
    "\n",
    "A biblioteca Lasagne disponibiliza duas funções úteis para salvar / carregar modelos pré-treinados:\n",
    "\n",
    "```\n",
    "params = lasagne.layers.get_all_param_values(net['out'])\n",
    "```\n",
    "    * Retorna uma lista com o valor de todos os parâmetros do modelo\n",
    "\n",
    "```\n",
    "lasagne.layers.set_all_param_values(net['out'], params)\n",
    "```\n",
    "    * Atualiza todos os parâmetros do modelo, usando a lista de parametros informada.\n",
    "\n",
    "Para salvarmos em disco, podemos usar a bibloteca \"pickle\". Essa biblioteca permite serializar qualquer objeto e escrevê-lo / carregá-lo do disco. \n",
    "\n",
    "```\n",
    "cPickle.dump(variavel, open(nome_de_arquivo, 'w'))\n",
    "```\n",
    "\n",
    "    * Serializa e salva a variavel\n",
    "\n",
    "```\n",
    "variavel = cPickle.load(open(nome_de_arquivo))\n",
    "```\n",
    "\n",
    "    * Carrega uma variável do disco.\n",
    "\n",
    "\n",
    "Abaixo temos um exemplo completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle \n",
    "\n",
    "params = lasagne.layers.get_all_param_values(model_conv_larger['out'])\n",
    "cPickle.dump(params, open('conv_larger.pickle', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -lh 'conv_larger.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Criando uma nova rede:\n",
    "\n",
    "nova_rede = build_conv_larger()\n",
    "train_fn, valid_fn = compile_train_function(nova_rede, lr=0.1)\n",
    "\n",
    "acc = valid_fn(x_valid, y_valid)[1]\n",
    "print ('Taxa de acerto da nova rede: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que a \"nova_rede\" está inicializada aleatoriamente. Vamos agora carregar os parâmetros salvos em disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_params = cPickle.load(open('conv_larger.pickle'))\n",
    "lasagne.layers.set_all_param_values(nova_rede['out'], loaded_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = valid_fn(x_valid, y_valid)[1]\n",
    "print ('Taxa de acerto da nova rede: %.2f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que agora a nova_rede contém os pesos carregados do disco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios extras (avançado)\n",
    "\n",
    "## A.1) Randomizando os exemplos da base de treinamento\n",
    "\n",
    "No algoritmo mostrado acima, os exemplos são sempre utilizados na mesma ordem. Em particular, os mini-batches não variam à cada epoch (o exemplo 0 sempre é utilizado junto com os mesmo exemplos 1 à 31 para o cálculo do gradiente). Um mudança simples que melhora a convergência é a de re-ordenar os dados aleatoriamente no início de cada epoch.\n",
    "\n",
    "Altere a função \"iterate_minibatches\" para re-ordernar aleatoriamente os dados antes de começar a retornar cada mini-batch. Uma forma de implementar é criando um array de índices, e re-ordenar esse array aleatoriamente:\n",
    "\n",
    "```\n",
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "x[idx[0:10]], y[idx[0:10]] # retorna 10 elementos de X e y\n",
    "```\n",
    "\n",
    "\n",
    "## A.2) Learning Rate Variável\n",
    "\n",
    "Nesse tutorial, utilizamos uma \"learning rate\" fixa durante todo o treinamento. É recomendável dimininuir a Learning Rate ao longo do tempo (e.g. começar com 0.1 e terminar em 0.001) (exemplo, nesse [artigo](http://jmlr.org/proceedings/papers/v28/sutskever13.pdf)). Isso é facilmente implementado em lasagne, passando uma variável compartilhada do theano ao invés de um número:\n",
    "\n",
    "```\n",
    "lr = theano.shared(1)\n",
    "updates = lasagne.updates.sgd(loss, params, lr)\n",
    "```\n",
    "\n",
    "Que pode ser atualizada a qualquer momento do treinamento, por exemplo:\n",
    "```\n",
    "lr.set_value(0.9)\n",
    "```\n",
    "\n",
    "Modifique a função de treinamento para modificar a learning rate a cada epoch. \n",
    "Dica: Você pode utilizar a função \"np.linspace\" para obter um valor de learning rate a cada epoch. Exemplo:\n",
    "```\n",
    "np.linspace(0.1, 0.001, 10) # Valor inicial, valor final, número de epochs\n",
    "# retorna: array([ 0.1  ,  0.089,  0.078,  0.067,  0.056,  0.045,  0.034,  0.023,\n",
    "        0.012,  0.001])\n",
    "```\n",
    "\n",
    "## A.3) Early stopping\n",
    "\n",
    "Nos exemplos acima, fizemos o treinamento por um número fixo de iterações. Nesse método precisamos definir a priori o número de iterações (o que pode ser um problema, ou não). Observamos também, que em alguns casos, a performance da rede deteriora ao longo do tempo (a performance final em validação é pior que a performance em alguma outra epoch de treinamento). Esse segundo problema é mais importante, e fácil de se resolver: basta manter os parametros que obtiveram melhor performance em validacao. Segue pseudo-codigo:\n",
    "\n",
    "* best_params = None\n",
    "* best_val_cost = np.inf\n",
    "* (loop de treinamento)\n",
    "   * if val_cost < best_val_cost:\n",
    "      * best_val_cost = val_cost\n",
    "      * best_params = lasagne.layers.get_all_params_values(last_layer)\n",
    "\n",
    "Um algoritmo mais complexo, mas que também resolve a questão do número de iterações a serem executadas pode ser encontrado no livro de Deep Learning [link](http://www.deeplearningbook.org/contents/regularization.html): Algoritmo 7.1 na página 247\n",
    "\n",
    "\n",
    "## A.4) Data augmentation\n",
    "\n",
    "Uma tática muito utilizada em Deep Learning chama-se \"Data-augmentation\". Esse termo engloba técnicas para aumentar o tamanho da base de treinamento. Por exemplo, uma tática comum para visão computacional é incluir translações, rotações e mudanças de zoom nas imagens da base de treinamento.\n",
    "\n",
    "* Para tranlações, pode-se utilizar manipulação de arrays usando numpy\n",
    "* Para zoom (scaling), pode-se usar a função: scipy.ndimage.zoom\n",
    "* Para rotação, pode-se usar a função: scipy.ndimage.rotate\n",
    "\n",
    "Modifique a função de \"iterate_minibatches\" para incluir tais alterações na base de treinamento (com uma flag para desativar essas alterações para validação / test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Detalhes importantes não cobertos nesse tutorial\n",
    "\n",
    "* O uso de camadas não-determinísticas (e.g. dropout, batch normalization), requer um cuidado adicional: durante a validação e teste, queremos desabilitar a parte não determinística (que só é importante para treino). No lasagne, isso é feito passando o parametro \"deterministic=True\" na função \"get_outputs\". Por exemplo:\n",
    "\n",
    "```\n",
    "train_prediction = lasagne.layers.get_output(net['out'])\n",
    "test_prediction = lasagne.layers.get_output(net['out'], deterministic=True)\n",
    "```\n",
    "\n",
    "* Alguma camadas (e.g. batch normalization) possuem parâmetros que não são treináveis (e.g. representam a média e desvio padrão de um neurônio). Portanto, esses parâmetros não devem ser incluídos nos updates de treinamento. Isso é feito passando o parâmetro \"trainable=True\" na função \"get_all_params\"\n",
    "```\n",
    "params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links Úteis\n",
    "Curso de CNNs da Stanford: http://cs231n.stanford.edu/syllabus.html\n",
    "\n",
    "Livro de Deep Learning:\n",
    "http://www.deeplearningbook.org/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
