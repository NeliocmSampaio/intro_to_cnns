{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Theano\n",
    "\n",
    "Theano é uma biblioteca de computação simbólica, que possui várias funcionalidades que facilitam o treinamento de redes neurais (e outros modelos de Aprendizagem de Máquina): implementação de várias funcões de álgebra linear; execução em CPU ou GPU de forma transparente e, principamente, diferenciação automática (descrita abaixo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T    # Normalmente o módulo tensor é importado com o alias T\n",
    "theano.config.floatX = 'float64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis simbolicas\n",
    "\n",
    "Nessa biblioteca, cálculos são expressos como um grafo de computação. Por exemplo, vamos criar duas varíaveis escalares: a e b:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = T.scalar()\n",
    "b = T.scalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir c como a soma de a e b. O resultado é uma outra variável simbólica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = a + b\n",
    "type(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isto é, ao somar duas variáveis, não estamos imediatamente calculando o valor da soma, e sim construindo um grafo que descreve a função:\n",
    "\n",
    "![](images/simple_graph.svg)\n",
    "\n",
    "Para executar esse cálculo, precisamos primeiro compilar uma função. A sintaxe é a seguinte:\n",
    "\n",
    "```\n",
    "f = theano.function([lista_de_entradas], saida)\n",
    "```\n",
    "\n",
    "onde o primeiro argumento é uma lista de entradas da função (variáveis que são usadas para computar o resultado), e o segundo argumento é a variável (ou lista de variáveis) de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soma = theano.function([a,b],c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soma(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma alternativa para obter o resultado de uma expressão é usar a função eval, que imediatamente compila a função e a chama com os argumentos fornecidos (útil para Debug):\n",
    "\n",
    "```\n",
    "resultado = saida.eval({entrada1: valor, entrada2: valor})\n",
    "```\n",
    "\n",
    "onde **saida** é a variável simbólica que queremos computar, **entrada1/2** são as variáveis simbólicas de entrada, e **valor** são os valores assinalados para a variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.eval({a: 2, b: 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis compartilhadas (Shared Variables)\n",
    "\n",
    "Variáveis compartilhadas do Theano se comportam como variáveis simbólicas (podendo ser usadas em expressões simbólicas), e também como variáveis normais do Python (possuindo um \"estado\" permanente). \n",
    "\n",
    "Esse tipo de variável é muito útil para modelos de aprendizagem de máquina, onde queremos variáveis que mantenham um estado entre chamadas diferentes (por exemplo, os pesos de um modelo).\n",
    "\n",
    "Para criarmos uma variável compartilhada, usamos a seguinte função:\n",
    "\n",
    "```\n",
    "variavel = theano.shared(valor_inicial)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, considere um modelo linear, que computa a saída $y(x) = \\textbf{w}^\\intercal \\textbf{x}$, isto é, o produto entre os vetores w e x. \n",
    "\n",
    "Vamos iniciar a variável $\\textbf{w}$ com um vetor de duas posições: $[1, 0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "w_inicial = np.array([1., 0.5])\n",
    "\n",
    "w = theano.shared(w_inicial)\n",
    "print type(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que $w$ é uma variável simbólica, mas podemos obter seu valor usando a função:\n",
    "\n",
    "```\n",
    "variavel.get_value()\n",
    "```\n",
    "\n",
    "De forma similar, podemos atualizar seu valor utilizando:\n",
    "\n",
    "```\n",
    "variavel.set_value(novo_valor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('valor anterior: %s' % w.get_value())\n",
    "\n",
    "w.set_value([2, 0.5])\n",
    "print ('Novo valor: %s' % w.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo acima mostra que a variável compartilhada pode ser usada como uma variável normal em Python (mantendo um estado).\n",
    "\n",
    "O exemplo abaixo mostra como podemos usá-la em uma expressão simbólica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.vector()\n",
    "y = w.dot(x)    # a função dot implementa produto interno de vetores (e também matrizes)\n",
    "\n",
    "f = theano.function([x], y) # Note que w não é passado na função, pois é uma variável compartilhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f([5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde o resultado usa o valor atual da variável $w$: $  5 \\times 2 + 6 \\times 0.5 = 13$.\n",
    "\n",
    "Note que para compilarmos a função f, informamos apenas $x$ como entrada. Isto é, não precisamos informar as variáveis compartilhadas durante a compilação/chamada da função, visto que a variável mantém um estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Declare três variáveis:\n",
    "* a (variável compartilhada, com valor 3)\n",
    "* b (variável compartilhada, com valor 1)\n",
    "* x (variável simbólica)\n",
    "\n",
    "Implemente uma função que calcule $y = ax + b$. Compile a função para calcular esse valor, e execute com o valor de $x = 5$.\n",
    "\n",
    "Nota: nesse exercício, todas as variáveis são escalares (use T.scalar() para a variável $x$, theano.shared para $a$ e $b$)\n",
    "\n",
    "Nota 2: Esse [Link](http://www.deeplearning.net/software/theano/library/tensor/basic.html) contém a descrição das funcionalidades básicas do Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/theano_shared.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferenciação automática\n",
    "\n",
    "A grande vantagem de usar variáveis simbólicas é que isso permite diferenciação automática. \n",
    "Em vários problemas de Machine Learning, precisamos calcular a derivada parcial de uma função de custo, com relação a cada um dos parâmetros do modelo. Ao definir a função de custo como um grafo de operações, Theano pode então usar a regra da cadeia para automaticamente calcular tais derivadas.\n",
    "\n",
    "### Exemplo prático\n",
    "\n",
    "Consideremos a seguinte equação:\n",
    "\n",
    "$$c = a^2 + 3b$$\n",
    "\n",
    "Podemos usar o método T.grad para calcular as derivadas parciais $\\frac{\\delta c}{\\delta a}$ e $\\frac{\\delta c}{\\delta b}$\n",
    "\n",
    "A sintaxe do comando é:\n",
    "\n",
    "```\n",
    "dy_dx = T.grad(y, x)\n",
    "```\n",
    "\n",
    "Onde **y** é uma variável simbólica que representa a expressão a ser diferenciada, e **x** é a entrada. O resultado é outra variável simbólica, que contém a expressão para calcular $\\frac{\\delta y}{\\delta x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = T.scalar()\n",
    "b = T.scalar()\n",
    "\n",
    "c = T.pow(a,2) + 3 * b  # c = a^2 + 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_da = T.grad(c, a)\n",
    "dc_db = T.grad(c, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora criar uma função que retorna o valor das derivadas para dados valores de **a** e **b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = theano.function([a,b], [dc_da, dc_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g(5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem diferenciação automática, precisaríamos calcular manualmente as derivadas parciais:\n",
    "\n",
    "$$\\frac{\\delta c}{\\delta a} = 2a $$\n",
    "$$\\frac{\\delta c}{\\delta b} = 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('dc/da = %g' % (2 * 5))\n",
    "print('dc/db = %g' % 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício:\n",
    "\n",
    "Implemente a seguinte função em Theano:\n",
    "\n",
    "$$ y = \\log (2x) $$\n",
    "\n",
    "Use diferenciação automática para calcular\n",
    "\n",
    "$$ y'(10) $$\n",
    "\n",
    "Que indica a derivada de y respectivo à x, na posição $x = 10$\n",
    "\n",
    "Nota: Utilize a função T.log (a versão simbólica de np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua reposta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/theano_diff.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Aplicação: Regressão logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exercício, vamos implementar regressão logística para um problema em duas dimensões.\n",
    "\n",
    "Considere uma base de dados $\\mathcal{D} = \\{\\textbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, composto de N exemplos $(\\textbf{x},y)$, onde $\\textbf{x}$ é um vetor de entrada de 2 dimensões, e $y$ é a classe $y \\in \\{0, 1\\}$. \n",
    "\n",
    "O modelo de regressão logística é um modelo linear, onde a probabilidade do exemplo $x$ pertencer à classe $y = 1$ é dado por:\n",
    "\n",
    "$$ \\hat{y} = P(y=1 | x) = \\sigma(\\textbf{w}^\\intercal\\textbf{x} + b)$$\n",
    "\n",
    "Onde $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ é a função logística.\n",
    "\n",
    "O objetivo da regressão logística é otimizar os parâmetros $\\textbf{w}$ e $b$ de forma a maximizar a probabilidade da classe correta em cada exemplo da base de treinamento. Para tanto, devemos minimizar a seguinte função de custo:\n",
    "\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N{y^{(i)} \\log\\hat{y}^{(i)}  + (1-y^{(i)}) \\log(1 - \\hat{y}^{(i)}) }$$\n",
    "\n",
    "\n",
    "Nesse exercício, vamos otimizar os parametros $\\textbf{w}$ e $b$ usando descida de gradiente (Gradient Descent). Nesse método, calculamos a derivada parcial da função de custo com respeito aos parâmetros do modelo, e iterativamente os atualizamos, de acordo com a seguinte regra:\n",
    "\n",
    "$$\\textbf{w} = \\textbf{w} - \\alpha * \\frac{\\delta L}{\\delta \\textbf{w}}$$\n",
    "\n",
    "Onde $\\alpha$ é o tamanho do passo (Learning Rate), e $\\frac{\\delta L}{\\delta \\textbf{w}}$ é a derivada parcial da função de custo com respeito à $\\textbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, vamos usar uma base de dados sintética, que é criada abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "#Nesse exemplo, geramos duas classes de acordo com distribuições gaussianas, com parametros diferentes\n",
    "c1 = np.random.multivariate_normal(mean=(2,3), cov=[[2,-1],[-1,2]], size=(1000))\n",
    "c2 = np.random.multivariate_normal(mean=(5,6), cov=[[2,-1],[-1,1.5]], size=(1000))\n",
    "\n",
    "#concatenando em uma matrix X e vetor y\n",
    "data_x = np.concatenate((c1,c2))\n",
    "data_y = np.concatenate((np.ones(len(c1)), np.zeros(len(c2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Funções auxiliares para plotar gráficos dos dados e da função de decisão:\n",
    "def plot_data(c1,c2):\n",
    "    f,ax=plt.subplots(figsize=(6,5))\n",
    "    \n",
    "    ax.scatter(c1[:,0], c1[:,1], c = 'red', marker='+', s=20)\n",
    "    ax.scatter(c2[:,0], c2[:,1], c = 'white', s=20, edgecolor='b')\n",
    "    \n",
    "    ax.set_xlabel('lightness', fontsize=15)\n",
    "    ax.set_ylabel('width', fontsize=15)\n",
    "    ax.set_xlim([-1,9])\n",
    "    ax.set_ylim([-2,12])\n",
    "    return f,ax\n",
    "\n",
    "def plot_decision_boundary(c1,c2, w,b):\n",
    "    gety = lambda x: -(w[0] * x + b) / w[1]\n",
    "    f, ax = plot_data(c1,c2)\n",
    "    ax.plot([-2,10], [gety(-2),gety(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data(c1,c2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos dividir a base de dados em treinamento/validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = data_x.shape[0]\n",
    "\n",
    "indices = np.arange(N) # Cria uma sequencia 1,2,...,N\n",
    "np.random.shuffle(indices) # Permuta aleatóriamente a sequencia\n",
    "\n",
    "# Vamos separar em 80% para treinamento, 10% para validação, 10% para teste\n",
    "last_train_indice = int(0.8 * N)\n",
    "last_val_indice = int(0.9 * N)\n",
    "\n",
    "train_indices = indices[0:last_train_indice]\n",
    "val_indices = indices[last_train_indice: last_val_indice]\n",
    "test_indices = indices[last_val_indice:]\n",
    "\n",
    "x_train, y_train = data_x[train_indices], data_y[train_indices]\n",
    "x_val, y_val = data_x[val_indices], data_y[val_indices]\n",
    "x_test, y_test = data_x[test_indices], data_y[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do modelo\n",
    "\n",
    "O modelo logístico retorna predições usando a seguinte equação:\n",
    "\n",
    "$$ \\hat{y} =  \\sigma(\\textbf{w}^\\intercal\\textbf{x} + b)$$\n",
    "\n",
    "Vamos primeiro definir os valores iniciais para $\\textbf{w}$ e $b$:\n",
    "$$\\textbf{w} \\sim U(-0.001, 0.001) $$\n",
    "\n",
    "$$b = 0$$\n",
    "\n",
    "Onde U(a,b) retorna números aleatórios segundo uma distribuição uniforme entre $[a,b]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_init = np.random.uniform(-0.001,0.001, size=(2))\n",
    "b_init = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Nesse exercício, o objetivo é construir uma expressão simbólica que compute $\\hat{y}$. Vamos implementar esse cálculo para todos os exemplos da base de dados ao mesmo tempo, usando operações de matrizes. \n",
    "\n",
    "Nesse caso, consideramos como entrada uma matrix $X$, onde cada linha da matriz possui um exemplo (portanto, ela possui tamanho Nx2, onde N é o número de exemplos).\n",
    "\n",
    "Usando essa notação, podemos calcular $\\hat{y}$ para todos os vetores de entrada usando a seguinte expressão:\n",
    "$$\\textbf{z} = X\\textbf{w} + b$$\n",
    "$$ \\hat{\\textbf{y}} =  \\sigma(\\textbf{z})$$\n",
    "\n",
    "\n",
    "onde \n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "Para tanto, é necessário:\n",
    " * Definir as variáveis simbólicas $x$ (matrix), $w$ (vetor) e $b$ (escalar)\n",
    "     * Note que $\\textbf{w}$ e $b$ são variáveis compartilhadas. Use theano.shared, com w_init e b_init como inicializacao\n",
    " * Implementar as expressões acima para computar $\\textbf{z}$ e $\\hat{\\textbf{y}}$ \n",
    " \n",
    "Nota:\n",
    " * Para implementar a função sigmoid, utilize T.exp (a versão simbólica equivalente à np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "x = \n",
    "w = \n",
    "b = \n",
    "\n",
    "z =\n",
    "y_hat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a resposta\n",
    "\n",
    "%load solutions/theano_yhat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "O próximo passo é implementar a função de custo:\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N{y^{(i)} \\log\\hat{y}^{(i)}  + (1-y^{(i)}) \\log(1 - \\hat{y}^{(i)}) }$$\n",
    "\n",
    "Para tanto, é preciso:\n",
    "\n",
    "* Definir a variável simbólica $y$ (um vetor simbolico, que irá conter a classe correta de cada exemplo de treinamento)\n",
    "* Implementar a função de custo\n",
    "\n",
    "\n",
    "Dica: As operações aritméticas (soma, multiplicação, logaritmo), se aplicadas em um vetor, são executadas individualmente para cada elemento do vetor. É possível implementar a função acima sem nenhum loop:\n",
    "\n",
    "$$\\textbf{L}_\\text{vetor} = -\\big(\\textbf{y} \\log\\hat{\\textbf{y}}  + (1-\\textbf{y}) \\log(1 - \\hat{\\textbf{y}})\\big)$$\n",
    "\n",
    "$$ L = \\text{mean}(\\textbf{L}_\\text{vetor})$$\n",
    "\n",
    "```\n",
    "loss = (codigo para calcular o custo para cada elemento)\n",
    "loss = loss.mean() \n",
    "```\n",
    "\n",
    "Nota: utilize T.log, a versão simbólica de np.log para calcular o logaritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "y = \n",
    "loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Verificando a função de custo: para valores aleatórios, y_hat será 0.5 na média, \n",
    "#e portanto a função de custo deveria ser próxima a -log(0.5) ~=0.69\n",
    "\n",
    "loss.eval({x:np.random.random((100,2)), y:np.zeros(100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a resposta\n",
    "\n",
    "%load solutions/theano_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Para treinarmos o modelo, precisamos calcular a derivada parcial da função de custo referente à cada parametro do modelo. \n",
    "\n",
    "Use a função T.grad para calcular a derivada da variável ```loss``` relativo às variáveis $\\textbf{w}$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução:\n",
    "\n",
    "w_grad = \n",
    "b_grad = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a resposta\n",
    "\n",
    "%load solutions/theano_grad.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também calcular a taxa de acerto do modelo:\n",
    "\n",
    "Consideramos que o modelo prevê $y=1$ se a saída $\\hat{y} > 0.5$ e $y=0$ caso contrário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = T.ge(y_hat, 0.5)\n",
    "accuracy = T.eq(prediction, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos compilar a função que fará o treinamento.\n",
    "\n",
    "Seguindo o algoritmo de descida de gradiente, faremos a atualização dos parâmetros de acordo com a regra:\n",
    "$$ w = w - \\alpha \\nabla_w L$$\n",
    "\n",
    "onde $\\alpha$ é o tamanho do passo (também conhecido como Learning Rate).\n",
    "\n",
    "Ao compilarmos uma função do Theano, podemos passar uma lista de expressões a serem **atualizadas**, da seguinte forma:\n",
    "\n",
    "```\n",
    "updates = [\n",
    "   (variavel, novo_valor),\n",
    "   ...\n",
    "]\n",
    "```\n",
    "\n",
    "Dessa forma, a cada chamada da função, os valores de saída serão retornados **e** as variáveis serão atualizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = theano.shared(0.1)\n",
    "\n",
    "updates = [\n",
    "    (w, w - alpha  * w_grad),\n",
    "    (b, b - alpha * b_grad)\n",
    "]\n",
    "\n",
    "train_fn = theano.function([x, y], [loss, accuracy], updates = updates)\n",
    "valid_fn = theano.function([x, y], [loss, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, com a função de treinamento compilada, basta iterativamente chamá-la com os valores de $X$ e $y$ da base de treinamento, e os valores de $\\textbf{w}$ e $b$ serão automaticamente atualizados a cada chamada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando a fronteira de decisão com os parametros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(c1,c2, w.get_value(),b.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_history = []\n",
    "acc_history = []\n",
    "val_cost_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando o treinamento por 50 iterações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    cost, acc = train_fn(x_train,y_train) # w e b são atualizados aqui\n",
    "    cost_history.append(cost)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    val_cost, val_acc = valid_fn(x_val, y_val) # Essa chamada não muda os parametros\n",
    "    val_cost_history.append(val_cost)\n",
    "    val_acc_history.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(c1,c2, w.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando o treinamento por mais 3000 iterações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    cost, acc = train_fn(x_train,y_train)\n",
    "    cost_history.append(cost)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    val_cost, val_acc = valid_fn(x_val, y_val)\n",
    "    val_cost_history.append(val_cost)\n",
    "    val_acc_history.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(c1,c2, w.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(cost_history, 'b--', label='Treinamento')\n",
    "plt.plot(val_cost_history, 'r-', label='Validacao')\n",
    "plt.xlabel('Numero de iteracoes', fontsize=15)\n",
    "plt.ylabel('Custo', fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(acc_history, 'b--', label='Treinamento')\n",
    "plt.plot(val_acc_history, 'r-', label='Validacao')\n",
    "plt.xlabel('Numero de iteracoes', fontsize=15)\n",
    "plt.ylabel('Taxa de acerto', fontsize=15)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_preds = theano.function([x], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = np.mean((get_preds(x_test) > 0.5) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Acerto na base de teste: ', test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício - critério de parada\n",
    "\n",
    "No exercício acima, executamos o treinamento para um número fixo de iterações. Nesse exercício vamos implementar um outro critério de parada, baseado na convergência.\n",
    "\n",
    "O critério de parada será baseado no progresso de cada iteração. Considere a diminuição de erro entre duas iterações sucessivas:\n",
    "\n",
    "$$ \\Delta L^{(t)} = L^{(t-1)} - L^{(t)} $$\n",
    "\n",
    "Onde $L^{(t)}$ indica o resultado da função de custo (do treinamento) na iteração $t$.\n",
    "\n",
    "Vamos parar o algoritmo na iteração $t$ que satisfaça:\n",
    "\n",
    "$$ \\Delta L^{(t)} < \\epsilon $$ \n",
    "\n",
    "O pseudo-código da função de treinamento é:\n",
    "\n",
    "* funcao Treino(maximo_iteracoes, epsilon)\n",
    "   * para $t$ de 1 a maximo_iteracoes:\n",
    "       * $L^{(t)}$ = train_fn(x_train, y_train)\n",
    "       * $L_\\text{val}^{(t)}$ = valid_fn(x_val, y_val)\n",
    "       * $\\Delta t = L^{(t-1)} - L^{(t)}$\n",
    "       * se $\\Delta t < epsilon$\n",
    "            * Fim do loop\n",
    "            \n",
    "Implemente a função de treino acima. Ao fim do treinamento, imprima o número de iterações executadas\n",
    "\n",
    "\n",
    "Nota: cuidado para não fazer a verificação na primeira iteração (pois $L^{(t-1)}$ é indefinido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vamos re-iniciar os pesos do modelo, para os valores aleatórios iniciais:\n",
    "def reset_weights():\n",
    "    w.set_value(w_init)\n",
    "    b.set_value(b_init)\n",
    "reset_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sua solução\n",
    "\n",
    "def train(train_fn, valid_fn, x_train, y_train, x_val, y_val, max_iterations, epsilon):\n",
    "    cost_history = []  # historico de custo. cost_history[t] será o custo na iteracao t (L(t) no algoritmo acima)\n",
    "    val_cost_history = []\n",
    "    # Escreva aqui o codigo. Retorne uma lista do custo em treinamento e validação, \n",
    "    # como fizemos anteriormente usando \"cost_history\" e \"val_cost_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute essa célula para ver a solucao\n",
    "%load solutions/theano_parada.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos testar a função de treino com epsilon = 1e-5 = 0.00001\n",
    "\n",
    "reset_weights()\n",
    "cost_history, val_cost_history = train(train_fn, valid_fn, x_train, y_train, x_val, y_val, \n",
    "      max_iterations = 10000, \n",
    "      epsilon = 1e-5)\n",
    "\n",
    "print 'Funcao de custo em treino: %.4f' % (cost_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(c1,c2, w.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a função acima usando valores diferentes de epsilon (e.g. 1e-3, 1e-4) e note o numero de iterações para convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício extra (avançado)\n",
    "\n",
    "No exemplo acima, utilizamos o algoritmo de descida de gradiente para fazer a otimização.\n",
    "Alguns modelos (como redes neurais pequenas, ou regressão logística) podem ser otimizados de forma mais eficiente utilizando algoritmos de segunda ordem, como L-BFGS.\n",
    "\n",
    "Implemente a otimização do modelo de regressão logística usando o método scipy.optimize.fmin_bfgs ([manual](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html))\n",
    "\n",
    "Nota: para isso, você precisará definir uma função \"f\":\n",
    "\n",
    "```\n",
    "def f(params):\n",
    "    (...)\n",
    "    return custo, gradiente\n",
    "```\n",
    "\n",
    "* Argumento: um vetor com todos os parâmetros do modelo (valores de $w$ e $b$, com todos os valores em um único vetor)\n",
    "* Retorno: \n",
    " * \"custo\" (um escalar contendo o valor da função de custo para os valores de $w$ e $b$)\n",
    " * \"gradiente\" (um vetor contendo a derivada parcial da função de custo referente à cada parâmetro do modelo)\n",
    " \n",
    "Por exemplo, considere que $w$ contenha 2 valores, e $b$ contenha um valor. O vetor params conterá 3 valores; custo será um escalar, e o vetor gradiente conterá 3 valores.\n",
    "\n",
    "* Como você pode obter o valor de $w$ e $b$ a partir do vetor \"params\"? E para juntar os valores de  $\\frac{d_L}{d_w}$ e $\\frac{d_L}{d_b}$ no vetor \"gradiente\"?? \n",
    "* Como fazer para alterar as variáveis compartilhadas usando os valores informados de $w$ e $b$?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
