{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning usando lasagne - parte 2\n",
    "\n",
    "Nesse tutorial, vamos utilizar CNNs treinadas na base ImageNet para outros problemas de classificação de imagens, usando transfer learning.\n",
    "\n",
    "O objetivo é treinar um modelo para discriminar um subconjunto de 5 classes da base de dados Caltech 101 (http://www.vision.caltech.edu/Image_Datasets/Caltech101/). A base para esse exercício possui 325 imagens de tamanho 224x224x3. \n",
    "\n",
    "Na parte 2 desse tutorial, vamos utilizar o método de Finetunning descrito em [1]: \n",
    " 1. Vamos utilizar uma rede treinada na base ImageNet: https://github.com/Lasagne/Recipes/tree/master/modelzoo\n",
    " 2. Vamos remover a última camada da rede, adicionar novas camadas e prosseguir com o treinamento do modelo\n",
    "\n",
    "\n",
    "[1] Oquab, Maxime, et al. \"Learning and transferring mid-level image representations using convolutional neural networks.\" [link](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from train import train_minibatch, plot_train_curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset e pré-processamento\n",
    "\n",
    "Para esse exercício, vamos utilizar a mesma base de dados (5 classes da Caltech-101). \n",
    "\n",
    "Primeiramente, vamos carregar a base de dados, e executar os passos de pré-processamento que utilizamos na parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('caltech_5classes.npz')\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "classes = data['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = cPickle.load(open('vgg_cnn_s.pkl'))\n",
    "mean_img = params['mean image']\n",
    "\n",
    "def process_dataset(x):\n",
    "    x = np.transpose(x, (0,3,1,2))  # Modifica dados para: exemplos x canais RGB x altura x largura\n",
    "    x = x[:, ::-1]                  # Modifica canais de RGB para BGR\n",
    "    \n",
    "    x = x - mean_img\n",
    "    return x\n",
    "\n",
    "x_train = process_dataset(x_train)\n",
    "x_valid = process_dataset(x_valid)\n",
    "x_test = process_dataset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora implementar o método de fine-tunning. \n",
    "\n",
    "![](images/oquab.png)\n",
    "\n",
    "Para isso, precisamos dos seguintes passos:\n",
    "\n",
    "## Criação do novo modelo\n",
    "\n",
    "1. Criar o modelo de CNN que foi usado na base de dados origem (vgg_cnn_s)\n",
    "2. Carregar os pesos do modelo\n",
    "3. Alterar o modelo:\n",
    "   * Remover a última camada\n",
    "   * Adicionar uma ou mais camadas ao fim\n",
    "   \n",
    "## Treinamento\n",
    "* Fazer o fine-tunning the todas as camadas **ou**\n",
    "* Fazer o treinamento das últimas camadas que foram adicionadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do novo modelo\n",
    "\n",
    "Vamos começar importando o modelo já treinado, e verificando as camadas que ele possui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg_cnn_s_cpu import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício - alteração do modelo\n",
    "\n",
    "Vamos criar uma função que:\n",
    " * Carregue o modelo treinado na base origem (já está implementado abaixo)\n",
    " * Delete a última camada (fc8 e prob)\n",
    " * Crie uma nova camada (DenseLayer), com 5 saídas, e não-linearidade softmax. Chame-a de net['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.nonlinearities import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sua solução:\n",
    "\n",
    "\n",
    "def build_model_for_finetuning(params):\n",
    "    model = build_model()\n",
    "    lasagne.layers.set_all_param_values(model['prob'], params['values'])\n",
    "    \n",
    "    ## Coloque aqui seu código para deletar as últimas camadas e criar nova(s) camada(s). \n",
    "    # Nomeie a última camada como \"model['out']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/transfer_build_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model_for_finetuning(params)\n",
    "\n",
    "assert 'out' in model, 'Ultima camada deveria se chamar \"out\"'\n",
    "assert model['out'].input_layer == model['drop7'], 'Ultima camada deveria receber dados da camada \"drop7\"'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning - treinamento usando regularização\n",
    "\n",
    "Num primeiro momento, vamos considerar o caso de fazer o \"fine-tuning\" de todas as camadas. Isto é, a CNN é iniciada com os pesos aprendidos na base de origem, e agora faremos o treinamento na base destino, normalmente usando uma Learning Rate menor.\n",
    "\n",
    "Vamos usar a função de custo que utilizamos ontem (cross-entropy) com uma variação: vamos adicionar regularização.\n",
    "\n",
    "Regularização é importante para evitar over-fitting, e em particular é útil quando a base de dados é pequena (que é o nosso caso agora). A forma de regularização mais comum é conhecida como \"Norma L2\", que adiciona uma penalidade para valores grandes de $\\textbf{w}$:\n",
    "\n",
    "$$L_\\text{reg} = L + \\sum_i w_i^2$$\n",
    "\n",
    "Em lasagne, podemos implementá-lo da seguinte forma:\n",
    "\n",
    "```\n",
    "loss = ... #função para calcular a funcao de custo, como anteriormente\n",
    "loss += w_decay * regularize_layer_params(ultima_camada, l2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.regularization import l2, regularize_layer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício - implementando regularização L2\n",
    "\n",
    "Nesse exercício, vamos atualizar a função que compila a função de treinamento, para incluir regularização L2. Para isso, utilize a função ```regularize_layer_params``` para computar $\\sum_i w_i^2$, e some esse valor à função de custo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_train_function(net, lr, w_decay):\n",
    "    input_var = net['input'].input_var\n",
    "    output_var = T.ivector()\n",
    "\n",
    "    probs = lasagne.layers.get_output(net['out'], inputs=input_var)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(probs, output_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    ####\n",
    "    # insira aqui o código para adicionar regularização à função de custo\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    y_pred = T.argmax(probs, axis=1)\n",
    "    acc = T.eq(y_pred, output_var)\n",
    "    acc = acc.mean()\n",
    "    \n",
    "    test_probs = lasagne.layers.get_output(net['out'], inputs=input_var, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_probs, output_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_pred = T.argmax(test_probs, axis=1)\n",
    "    test_acc = T.eq(test_pred, output_var)\n",
    "    test_acc = test_acc.mean()\n",
    "\n",
    "    params = lasagne.layers.get_all_params(net['out'])\n",
    "    updates = lasagne.updates.sgd(loss, params, lr)\n",
    "\n",
    "    train_fn = theano.function([input_var, output_var], [loss, acc], updates=updates)\n",
    "    val_fn = theano.function([input_var, output_var], [test_loss, test_acc])\n",
    "    return train_fn, val_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que na função acima, usamos variáveis diferentes para a função de custo em treinamento e validação. Isso é importante para modelos que usem camadas não-determinísticas (e.g. Dropout), que é o caso do modelo atual. Essas camadas possuem comportamento diferente para treinamento e teste, portanto é importante obter as saídas de treinamento e validação da seguinte forma:\n",
    "\n",
    "```\n",
    "train_probs = lasagne.layers.get_output(net['out'], inputs=input_var)\n",
    "test_probs = lasagne.layers.get_output(net['out'], inputs=input_var, deterministic=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute para ver a solução\n",
    "\n",
    "%load solutions/transfer_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: finetuning\n",
    "\n",
    "* Crie um modelo chamando a função \"build_model_for_finetuning\".\n",
    "* Compile as funções de treinamento usando a função acima, com lr=0.001 e w_decay= 1e-5\n",
    "* Execute a célula seguinte para efetuar o treinamento (levará vários minutos para treinar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sua solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/transfer_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_curves = train_minibatch(train_fn, valid_fn,    \n",
    "                     train_set=(x_train, y_train), \n",
    "                     valid_set=(x_test, y_test),\n",
    "                     epochs=20,\n",
    "                     batch_size=16)\n",
    "plot_train_curves(train_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A performance em aceitação é razoável, mas notamos que o modelo entra em overfitting - principalmente causado pelo pequeno tamanho da base de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-treinando apenas algumas camadas\n",
    "\n",
    "Vamos agora considerar o caso de re-treinar apenas um sub-conjunto de camadas (geralmente, as últimas camadas adicionadas ao modelo)\n",
    "\n",
    "### Exercício: treinando apenas algumas camadas\n",
    "\n",
    "Modifique a função de treinamento abaixo, para que o treinamento atualize apenas os pesos de algumas camadas da rede.\n",
    "\n",
    "Isto é, ao invés de utilizar a função ```lasagne.layers.get_all_params``` para obter a lista de parametros, vamos contruir a lista de parâmetros manualmente. Dada uma lista de camadas que desejamos treinar, agregue todos os parâmetros dessas camadas em uma lista chamada \"params\"\n",
    "\n",
    "Dica: utilize a função abaixo [manual](http://lasagne.readthedocs.io/en/latest/modules/layers/base.html#lasagne.layers.Layer.get_params)\n",
    "\n",
    "```\n",
    "camada.get_params(treinable=True)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_train_function_somelayers(net, lr, w_decay, layers_to_train):\n",
    "    input_var = net['input'].input_var\n",
    "    output_var = T.ivector()\n",
    "\n",
    "    probs = lasagne.layers.get_output(net['out'], inputs=input_var)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(probs, output_var)\n",
    "    loss = loss.mean()\n",
    "    loss += w_decay * regularize_layer_params(net['out'], l2)\n",
    "    \n",
    "    y_pred = T.argmax(probs, axis=1)\n",
    "    acc = T.eq(y_pred, output_var)\n",
    "    acc = acc.mean()\n",
    "    \n",
    "    test_probs = lasagne.layers.get_output(net['out'], inputs=input_var, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_probs, output_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_pred = T.argmax(test_probs, axis=1)\n",
    "    test_acc = T.eq(test_pred, output_var)\n",
    "    test_acc = test_acc.mean()\n",
    "    \n",
    "    ####\n",
    "    # Adicione aqui o codigo para criar uma lista chamada \"params\" com os \n",
    "    # parametros de todas as camadas da lista \"layers_to_train\"\n",
    "    \n",
    "    ####\n",
    "        \n",
    "    updates = lasagne.updates.sgd(loss, params, lr)\n",
    "\n",
    "    train_fn = theano.function([input_var, output_var], [loss, acc], updates=updates)\n",
    "    val_fn = theano.function([input_var, output_var], [test_loss, test_acc])\n",
    "    return train_fn, val_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inicialmente treinar apenas a última camada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model_for_finetuning(params)\n",
    "\n",
    "train_fn, valid_fn = compile_train_function_somelayers(model, lr=0.005, w_decay=1e-5, layers_to_train=[model['out']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Forçar a liberação de memória do Garbage Collector - para evitar problemas de memória\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_curves = train_minibatch(train_fn, valid_fn,     # Treinamento usando Batch Gradient Descent\n",
    "                     train_set=(x_train, y_train), \n",
    "                     valid_set=(x_test, y_test),\n",
    "                     epochs=20,\n",
    "                     batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_train_curves(train_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: treinando as duas ultimas camadas\n",
    "\n",
    "* Crie um modelo usando ```build_model_for_finetuning(params)```\n",
    "* Treine as duas últimas camadas, usando lr=0.005, w_decay=1e-5, por 20 epochs e batch_size=16\n",
    "* Mostre o gráfico da curva de aprendizagem, usando a função ```plot_train_curves```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sua solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/transfer_train_twolayers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que nesse exercício usamos uma base de destino pequena. O que você observa ao treinar todas as camadas vs treinar somente a nova camada nesse cenário?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios extras:\n",
    "\n",
    "* A.1 Adicionar mais camadas ao fim da rede - verificar a performance treinando apenas essas camadas, ou todas as camadas da rede"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
